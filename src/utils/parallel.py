"""
å¹¶å‘å¤„ç†æ¨¡å—
æä¾›å¹¶è¡Œå¤„ç†åŠŸèƒ½ï¼Œæå‡æ‰¹é‡å¤„ç†æ•ˆçŽ‡
"""
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Callable, Any, Tuple, Optional
import threading
from functools import wraps

from src.utils.logger import get_logger

logger = get_logger(__name__)


def parallel_process(
    items: List[Any],
    func: Callable,
    max_workers: Optional[int] = None,
    use_processes: bool = False,
    timeout: Optional[float] = None
) -> List[Tuple[Any, Any, Optional[Exception]]]:
    """
    å¹¶è¡Œå¤„ç†å¤šä¸ªé¡¹ç›®
    
    Args:
        items: è¦å¤„ç†çš„é¡¹ç›®åˆ—è¡¨
        func: å¤„ç†å‡½æ•°ï¼ŒæŽ¥å—å•ä¸ªé¡¹ç›®ä½œä¸ºå‚æ•°ï¼Œè¿”å›žå¤„ç†ç»“æžœ
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        use_processes: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆé»˜è®¤ä½¿ç”¨çº¿ç¨‹æ± ï¼‰
        timeout: å•ä¸ªä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        List[Tuple[item, result, error]]: å¤„ç†ç»“æžœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(åŽŸå§‹é¡¹ç›®, å¤„ç†ç»“æžœ, å¼‚å¸¸)
    """
    if not items:
        return []
    
    # ç¡®å®šæœ€å¤§å¹¶å‘æ•°
    if max_workers is None:
        max_workers = min(len(items), 5)  # é»˜è®¤æœ€å¤š5ä¸ªå¹¶å‘
    
    results = []
    executor_class = ProcessPoolExecutor if use_processes else ThreadPoolExecutor
    
    with executor_class(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_item = {
            executor.submit(func, item): item
            for item in items
        }
        
        # æ”¶é›†ç»“æžœ
        for future in as_completed(future_to_item, timeout=timeout):
            item = future_to_item[future]
            try:
                result = future.result(timeout=timeout)
                results.append((item, result, None))
                logger.debug(f"âœ… å¤„ç†å®Œæˆ: {item}")
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ {item}: {e}", exc_info=True)
                results.append((item, None, e))
    
    return results


def parallel_map(
    items: List[Any],
    func: Callable,
    max_workers: Optional[int] = None,
    use_processes: bool = False
) -> List[Any]:
    """
    å¹¶è¡Œæ˜ å°„å¤„ç†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œåªè¿”å›žç»“æžœï¼‰
    
    Args:
        items: è¦å¤„ç†çš„é¡¹ç›®åˆ—è¡¨
        func: å¤„ç†å‡½æ•°
        max_workers: æœ€å¤§å¹¶å‘æ•°
        use_processes: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± 
    
    Returns:
        List[Any]: å¤„ç†ç»“æžœåˆ—è¡¨ï¼ˆå¤±è´¥çš„é¡¹ç›®è¿”å›žNoneï¼‰
    """
    results = parallel_process(items, func, max_workers, use_processes)
    return [result if error is None else None for _, result, error in results]


class ThreadSafeCounter:
    """çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨"""
    def __init__(self, initial_value: int = 0):
        self._value = initial_value
        self._lock = threading.Lock()
    
    def increment(self, amount: int = 1) -> int:
        """å¢žåŠ è®¡æ•°å¹¶è¿”å›žæ–°å€¼"""
        with self._lock:
            self._value += amount
            return self._value
    
    def get(self) -> int:
        """èŽ·å–å½“å‰å€¼"""
        with self._lock:
            return self._value
    
    def reset(self):
        """é‡ç½®è®¡æ•°å™¨"""
        with self._lock:
            self._value = 0


def batch_process(
    items: List[Any],
    func: Callable,
    batch_size: int = 5,
    max_workers: Optional[int] = None,
    delay_between_batches: float = 0
) -> List[Tuple[Any, Any, Optional[Exception]]]:
    """
    åˆ†æ‰¹å¹¶å‘å¤„ç†ï¼ˆé¿å…è¿‡å¤šå¹¶å‘å¯¼è‡´APIé™åˆ¶ï¼‰
    
    Args:
        items: è¦å¤„ç†çš„é¡¹ç›®åˆ—è¡¨
        func: å¤„ç†å‡½æ•°
        batch_size: æ¯æ‰¹å¤„ç†çš„æ•°é‡
        max_workers: æ¯æ‰¹çš„æœ€å¤§å¹¶å‘æ•°
        delay_between_batches: æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    Returns:
        List[Tuple[item, result, error]]: å¤„ç†ç»“æžœåˆ—è¡¨
    """
    all_results = []
    total = len(items)
    
    for i in range(0, total, batch_size):
        batch = items[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total + batch_size - 1) // batch_size
        
        logger.info(f"ðŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªé¡¹ç›®)")
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        batch_results = parallel_process(batch, func, max_workers)
        all_results.extend(batch_results)
        
        # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆæœ€åŽä¸€æ‰¹ä¸éœ€è¦å»¶è¿Ÿï¼‰
        if delay_between_batches > 0 and i + batch_size < total:
            import time
            logger.debug(f"ðŸ’¤ æ‰¹æ¬¡é—´å»¶è¿Ÿ {delay_between_batches} ç§’...")
            time.sleep(delay_between_batches)
    
    return all_results


def retry_on_failure(max_retries: int = 3, delay: float = 1):
    """
    é‡è¯•è£…é¥°å™¨
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        logger.warning(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ{delay}ç§’åŽé‡è¯•: {e}")
                        import time
                        time.sleep(delay)
                    else:
                        logger.error(f"âŒ é‡è¯• {max_retries} æ¬¡åŽä»å¤±è´¥: {e}")
            raise last_exception
        return wrapper
    return decorator
