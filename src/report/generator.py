"""
æŠ¥å‘Šç”Ÿæˆæ¨¡å—
æ‰¹é‡å¤„ç†è‚¡ç¥¨å¹¶ç”ŸæˆPDFæŠ¥å‘Š
"""

import os
import re
import shutil
import time
import zipfile
from datetime import datetime
from typing import List, Tuple, Optional, Dict, Any

import pandas as pd

from src.utils.code_normalizer import normalize_code, parse_stock_list
from src.data.fetchers import (
    get_name,
    fetch_kline_data,
    get_market_indices_data,
    get_sector_indices_data,
    load_sector_index_map,
    normalize_beijing_time,
    filter_trading_hours,
)
from src.analysis import calculate_technical_indicators, resample_kline_data
from src.visualization import create_candle_chart, create_indices_charts, create_pdf_with_market_analysis
from src.config import Config
from src.utils.logger import get_logger
from src.utils.parallel import parallel_process, batch_process
from src.utils.exceptions import (
    DataFetchError,
    IndicatorCalculationError,
    ReportGenerationError,
)

logger = get_logger(__name__)


def _process_single_stock(
    code_input: str, output_folder: str, sector_input: Optional[str], sector_map: Dict[str, Any], index: int, total: int
) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:
    """
    å¤„ç†å•ä¸ªè‚¡ç¥¨ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œç”¨äºå¹¶å‘å¤„ç†ï¼‰

    Args:
        code_input: è‚¡ç¥¨ä»£ç è¾“å…¥
        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
        sector_input: è¡Œä¸šè¾“å…¥
        sector_map: è¡Œä¸šæ˜ å°„å­—å…¸
        index: å½“å‰ç´¢å¼•
        total: æ€»æ•°

    Returns:
        Tuple[stock_code, stock_name, pdf_path, error]: å¤„ç†ç»“æœ
    """
    try:
        logger.info(f"\n" + "=" * 70)
        logger.info(f"ç¬¬ {index}/{total} ä¸ªè‚¡ç¥¨: {code_input}")
        logger.info("=" * 70)

        if not code_input:
            logger.warning("âš ï¸  è·³è¿‡ç©ºä»£ç ")
            return (None, None, None, "ç©ºä»£ç ")

        name_to_code = sector_map.get("name_to_code", {})
        code_to_name = sector_map.get("code_to_name", {})

        is_sector_input = False
        stock_code = None
        stock_name = None

        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºè¡Œä¸šä»£ç ï¼ˆBKå¼€å¤´ï¼‰
        if code_input.startswith("BK") and code_input in code_to_name:
            stock_code = code_input
            stock_name = code_to_name[code_input]
            is_sector_input = True
            logger.info(f"â„¹ï¸  è¯†åˆ«ä¸ºè¡Œä¸šä»£ç : {stock_code} ({stock_name})")

        # 2. æ£€æŸ¥æ˜¯å¦ä¸ºè¡Œä¸šåç§°ï¼ˆå®Œå…¨åŒ¹é…ï¼‰
        elif code_input in name_to_code:
            stock_code = name_to_code[code_input]
            stock_name = code_input
            is_sector_input = True
            logger.info(f"â„¹ï¸  è¯†åˆ«ä¸ºè¡Œä¸šåç§°: {stock_name} ({stock_code})")

        # 3. æ¨¡ç³ŠåŒ¹é…è¡Œä¸šåç§°
        if not is_sector_input:
            potential_name = code_input.split(".")[0] if "." in code_input else code_input
            for s_name, s_code in name_to_code.items():
                if len(potential_name) >= 2 and (potential_name in s_name or s_name in potential_name):
                    stock_code = s_code
                    stock_name = s_name
                    is_sector_input = True
                    logger.info(f"â„¹ï¸  æ¨¡ç³ŠåŒ¹é…åˆ°è¡Œä¸š: {stock_name} ({stock_code})")
                    break

        # 4. å¦‚æœä»ç„¶ä¸æ˜¯è¡Œä¸šï¼Œåˆ™ä½œä¸ºæ™®é€šè‚¡ç¥¨å¤„ç†
        if not is_sector_input:
            stock_code = normalize_code(code_input)
            stock_name = get_name(stock_code)
            logger.info(f"ğŸ“ˆ è¯†åˆ«ä¸ºè‚¡ç¥¨: {stock_code} ({stock_name or 'æœªçŸ¥'})")

        if not stock_name:
            stock_name = "æœªçŸ¥è‚¡ç¥¨" if not is_sector_input else "æœªçŸ¥è¡Œä¸š"

        timestamp = datetime.now().strftime("%H%M%S")
        temp_dir = os.path.join(output_folder, f"temp_{stock_code}_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)

        logger.info("\n1ï¸âƒ£  è·å–å¸‚åœºæŒ‡æ•°æ•°æ®...")
        is_hk = str(stock_code).startswith("HK.")
        indices_data = get_market_indices_data(is_hk=is_hk)

        # è·å–è¡Œä¸šæ¿å—æŒ‡æ•°
        current_sector = sector_input or (stock_code if is_sector_input else None)
        if current_sector:
            logger.info(f"   è·å–è¡Œä¸šæ¿å—æŒ‡æ•°: {current_sector}")
            sector_indices_data = get_sector_indices_data(current_sector, count=150)
            if sector_indices_data:
                indices_data.update(sector_indices_data)

        logger.info("\n2ï¸âƒ£  è·å–æ•°æ®...")
        stock_data_map = {}
        data_source = "AKShare(è¡Œä¸š)" if is_sector_input else ("æ–°æµªè´¢ç»/ä¸œæ–¹è´¢å¯Œ" if is_hk else "æ–°æµªè´¢ç»")

        # ä»é…ç½®åŠ è½½æŒ‡æ ‡å‚æ•°
        config = Config()
        indicator_params = config.indicator_params

        report_meta = {
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "data_source": data_source,
            "index_source": config.get("data_sources.index_source", "æ–°æµªè´¢ç»"),
            "indicator_params": indicator_params,
        }

        # æ•°æ®æŠ“å–
        try:
            if is_sector_input:
                import akshare as ak

                df_day = ak.stock_board_industry_hist_em(
                    symbol=stock_name, period="daily", start_date="20230101", end_date="20261231", adjust=""
                )
                if df_day is not None and not df_day.empty:
                    df_day = df_day.rename(
                        columns={
                            "æ—¥æœŸ": "Date",
                            "å¼€ç›˜": "Open",
                            "æ”¶ç›˜": "Close",
                            "æœ€é«˜": "High",
                            "æœ€ä½": "Low",
                            "æˆäº¤é‡": "Volume",
                        }
                    )
                    df_day["Date"] = pd.to_datetime(df_day["Date"])
                    df_day.set_index("Date", inplace=True)
                    df_day = calculate_technical_indicators(df_day)
                    stock_data_map["day"] = df_day
                    stock_data_map["week"] = resample_kline_data(df_day, "W")
                    stock_data_map["month"] = resample_kline_data(df_day, "M")

                    for p in ["30", "5"]:
                        for retry in range(3):
                            try:
                                df_min = ak.stock_board_industry_hist_min_em(symbol=stock_name, period=p)
                                if df_min is not None and not df_min.empty:
                                    df_min = df_min.rename(
                                        columns={
                                            "æ—¶é—´": "Date",
                                            "å¼€ç›˜": "Open",
                                            "æ”¶ç›˜": "Close",
                                            "æœ€é«˜": "High",
                                            "æœ€ä½": "Low",
                                            "æˆäº¤é‡": "Volume",
                                        }
                                    )
                                    df_min["Date"] = pd.to_datetime(df_min["Date"])
                                    df_min.set_index("Date", inplace=True)
                                    df_min = calculate_technical_indicators(df_min)
                                    stock_data_map[f"{p}m"] = df_min
                                    break
                            except:
                                time.sleep(2)
            else:
                df_day = fetch_kline_data(stock_code, 240, 150)
                if df_day is not None:
                    df_day = calculate_technical_indicators(df_day)
                    stock_data_map["day"] = df_day
                    stock_data_map["week"] = resample_kline_data(df_day, "W")
                    stock_data_map["month"] = resample_kline_data(df_day, "M")

                    for p in [30, 5, 1]:
                        df_min = fetch_kline_data(stock_code, p, 100)
                        if df_min is not None:
                            if not is_hk:
                                df_min = normalize_beijing_time(df_min)
                                df_min = filter_trading_hours(df_min)
                            df_min = calculate_technical_indicators(df_min)
                            stock_data_map[f"{p}m" if p != 1 else "1m"] = df_min
        except DataFetchError as e:
            logger.error("æ•°æ®è·å–å¤±è´¥ %s: %s", stock_code, e)
            return (stock_code, stock_name, None, f"æ•°æ®è·å–å¤±è´¥: {e}")
        except IndicatorCalculationError as e:
            logger.error("æŒ‡æ ‡è®¡ç®—å¤±è´¥ %s: %s", stock_code, e)
            return (stock_code, stock_name, None, f"æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        except Exception as e:
            logger.error("æ•°æ®è·å–å¼‚å¸¸ %s: %s", stock_code, e, exc_info=True)

        if "day" not in stock_data_map or stock_data_map["day"] is None:
            logger.error(f"âŒ æ— æ³•è·å–æ ¸å¿ƒæ•°æ®ï¼Œè·³è¿‡ {stock_code}")
            return (stock_code, stock_name, None, "æ— æ•°æ®")

        logger.info(f"\n3ï¸âƒ£  ç”Ÿæˆå›¾è¡¨...")
        create_indices_charts(indices_data, temp_dir)

        chart_config = config.chart_config
        max_points = chart_config.get("max_points", {})

        chart_configs = [
            ("day", stock_data_map.get("day"), f"{stock_name} æ—¥çº¿", max_points.get("day", 60)),
            ("week", stock_data_map.get("week"), f"{stock_name} å‘¨çº¿", max_points.get("week", 60)),
            ("month", stock_data_map.get("month"), f"{stock_name} æœˆçº¿", max_points.get("month", 60)),
            ("30m", stock_data_map.get("30m"), f"{stock_name} 30åˆ†é’Ÿ", max_points.get("minute", 100)),
            ("5m", stock_data_map.get("5m"), f"{stock_name} 5åˆ†é’Ÿ", max_points.get("minute", 100)),
            ("1m", stock_data_map.get("1m"), f"{stock_name} 1åˆ†é’Ÿ", max_points.get("minute", 100)),
        ]
        for key, df, title, max_points in chart_configs:
            if df is not None and len(df) >= 5:
                create_candle_chart(df, title, os.path.join(temp_dir, f"{key}.png"), max_points=max_points)

        logger.info("\n4ï¸âƒ£  ç”ŸæˆPDFæŠ¥å‘Š...")
        safe_name = re.sub(r'[\\/*?:"<>|]', "_", stock_name)
        pdf_path = os.path.join(output_folder, f"{safe_name}_{stock_code}_å¢å¼ºåˆ†ææŠ¥å‘Š.pdf")
        stock_data_map["_meta"] = report_meta

        try:
            ok = create_pdf_with_market_analysis(
                stock_code, stock_name, stock_data_map, indices_data, pdf_path, temp_dir
            )
        except ReportGenerationError as e:
            logger.error("PDFç”Ÿæˆå¤±è´¥ %s (%s): %s", stock_code, stock_name, e)
            try:
                shutil.rmtree(temp_dir)
            except Exception:
                pass
            return (stock_code, stock_name, None, "PDFç”Ÿæˆå¤±è´¥")
        if ok:
            logger.info("æŠ¥å‘Šå·²ç”Ÿæˆ: %s", pdf_path)
            try:
                shutil.rmtree(temp_dir)
            except Exception:
                pass
            return (stock_code, stock_name, pdf_path, None)
        logger.error("PDFç”Ÿæˆå¤±è´¥ %s (%s)", stock_code, stock_name)
        try:
            shutil.rmtree(temp_dir)
        except Exception:
            pass
        return (stock_code, stock_name, None, "PDFç”Ÿæˆå¤±è´¥")

    except (DataFetchError, IndicatorCalculationError, ReportGenerationError) as e:
        logger.error("å¤„ç†å¤±è´¥ %s: %s", code_input, e)
        return (None, None, None, str(e))
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è‚¡ç¥¨å¤±è´¥ {code_input}: {e}", exc_info=True)
        return (None, None, None, str(e))


def process_multiple_stocks(
    stock_codes_input: str, output_folder: str, sector_input: Optional[str] = None
) -> Tuple[List[Tuple], List[Tuple]]:
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªè‚¡ç¥¨

    Args:
        stock_codes_input: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆç©ºæ ¼æˆ–é€—å·åˆ†éš”ï¼‰
        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
        sector_input: è¡Œä¸šä»£ç ï¼ˆå¦‚"BK1031"ï¼‰æˆ–è¡Œä¸šåç§°ï¼ˆå¦‚"å…‰ä¼è®¾å¤‡"ï¼‰ï¼Œå¯é€‰

    Returns:
        Tuple[List[Tuple], List[Tuple]]: (æˆåŠŸæŠ¥å‘Šåˆ—è¡¨, å¤±è´¥æŠ¥å‘Šåˆ—è¡¨)
    """
    stock_codes = parse_stock_list(stock_codes_input)
    logger.info(f"ğŸ“Š æ‰¹é‡åˆ†æ {len(stock_codes)} ä¸ªè‚¡ç¥¨")

    successful_reports = []
    failed_reports = []

    # åŠ è½½è¡Œä¸šæ˜ å°„
    sector_map = load_sector_index_map()

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶å‘å¤„ç†
    config = Config()
    parallel_config = config.get("parallel", {})
    use_parallel = parallel_config.get("enabled", False) and len(stock_codes) > 1

    if use_parallel:
        # ä½¿ç”¨å¹¶å‘å¤„ç†
        max_workers = parallel_config.get("max_workers", 3)
        batch_size = parallel_config.get("batch_size", 5)
        delay_between_batches = parallel_config.get("delay_between_batches", 1)

        logger.info(f"ğŸš€ å¯ç”¨å¹¶å‘å¤„ç†: æœ€å¤§å¹¶å‘æ•°={max_workers}, æ‰¹æ¬¡å¤§å°={batch_size}")

        # å‡†å¤‡å¤„ç†ä»»åŠ¡
        tasks = [
            (code_input, output_folder, sector_input, sector_map, i + 1, len(stock_codes))
            for i, code_input in enumerate(stock_codes)
        ]

        # ä½¿ç”¨åˆ†æ‰¹å¹¶å‘å¤„ç†
        results = batch_process(
            tasks,
            lambda task: _process_single_stock(*task),
            batch_size=batch_size,
            max_workers=max_workers,
            delay_between_batches=delay_between_batches,
        )

        # å¤„ç†ç»“æœï¼ˆresultsæ ¼å¼: [(task, result, error), ...]ï¼‰
        for task, result, error in results:
            code_input = task[0]  # ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯code_input
            if error is None and result is not None:
                stock_code, stock_name, pdf_path, result_error = result
                if result_error is None and pdf_path:
                    successful_reports.append((stock_code, stock_name, pdf_path))
                else:
                    failed_reports.append((stock_code or code_input, stock_name or "æœªçŸ¥", result_error or "å¤„ç†å¤±è´¥"))
            else:
                error_msg = str(error) if error else "å¤„ç†å¤±è´¥"
                failed_reports.append((code_input, "æœªçŸ¥", error_msg))
    else:
        # ä½¿ç”¨ä¸²è¡Œå¤„ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        logger.info("ğŸ“ ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼")

        for i, code_input in enumerate(stock_codes, 1):
            stock_code, stock_name, pdf_path, error = _process_single_stock(
                code_input, output_folder, sector_input, sector_map, i, len(stock_codes)
            )

            if error is None and pdf_path:
                successful_reports.append((stock_code, stock_name, pdf_path))
            else:
                failed_reports.append((stock_code or code_input, stock_name or "æœªçŸ¥", error or "å¤„ç†å¤±è´¥"))

            # ä¸²è¡Œæ¨¡å¼ä¸‹çš„å»¶è¿Ÿï¼ˆå¹¶å‘æ¨¡å¼ä¸‹ç”±batch_processå¤„ç†ï¼‰
            if i < len(stock_codes):
                delays = config.delays
                if stock_code and str(stock_code).startswith("HK."):
                    delay = delays.get("hk_stock", 3)
                else:
                    delay = delays.get("normal", 1)
                logger.debug(f"ğŸ’¤ ç­‰å¾… {delay} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªè‚¡ç¥¨...")
                time.sleep(delay)

    return successful_reports, failed_reports


def create_zip_archive(reports_folder: str, zip_filename: Optional[str] = None) -> Optional[str]:
    """åˆ›å»ºZIPå‹ç¼©åŒ…

    Args:
        reports_folder: æŠ¥å‘Šæ–‡ä»¶å¤¹è·¯å¾„
        zip_filename: ZIPæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰

    Returns:
        Optional[str]: ZIPæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    logger = get_logger(__name__)

    if not os.path.exists(reports_folder) or not os.listdir(reports_folder):
        logger.warning(f"âš ï¸  æŠ¥å‘Šæ–‡ä»¶å¤¹ä¸ºç©ºæˆ–ä¸å­˜åœ¨: {reports_folder}")
        return None

    if zip_filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"stock_reports_{timestamp}.zip"

    zip_path = os.path.join(reports_folder, zip_filename)

    try:
        with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(reports_folder):
                for file in files:
                    if file.endswith(".pdf"):
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, reports_folder)
                        zipf.write(file_path, arcname)
                        logger.debug(f"ğŸ“¦ æ·»åŠ æ–‡ä»¶åˆ°ZIP: {arcname}")

        zip_size = os.path.getsize(zip_path) / (1024 * 1024)
        logger.info(f"âœ… ZIPå‹ç¼©åŒ…åˆ›å»ºæˆåŠŸ: {zip_path}")
        logger.info(f"ğŸ“¦ å‹ç¼©åŒ…å¤§å°: {zip_size:.2f} MB")

        return zip_path

    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºZIPå‹ç¼©åŒ…å¤±è´¥: {e}", exc_info=True)
        return None
